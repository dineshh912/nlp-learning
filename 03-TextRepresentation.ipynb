{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Representation\n",
    "\n",
    "Transform a pre-processed text into suitable numerical form and fed into ML algorithm for further process is called feature extraction or text representation.\n",
    "Feature extraction is common step in any ml problem such as image, video, audio. \n",
    "\n",
    " - Images will be transform into matrix representation based on their pixel values.\n",
    " - Video also similar, video is just a collection of frames where each frame is an image.  so the video represent as a sequential collection of matrices. \n",
    " - Audio usually transmit as waves. so represent this mathematically, sampled wave amplitude will be \n",
    "recorded. this will give array representation of the sound waves.\n",
    "\n",
    "Text representation approach classified into 4 categories\n",
    "\n",
    " - Basic vectorization approaches\n",
    " - Distributed representations\n",
    " - Universal language representation\n",
    " - Handcrafted Features\n",
    "\n",
    "##### Text which represent by vectors of numbers is called vector space model. It's simple model used for representing any text blob. It's fundamental to many NLP operations like info-retrieval, scoring the documents etc.,  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic vectorization approaches\n",
    "Match each word in the vocabulary of the text corpus to a unique ID(integer). Then represent sentence in the corpus as a v-dimensional vector. \n",
    "\n",
    "### One- Hot Encoding\n",
    "In this method, each word w in the corpus given a unique integer ID, It's between 1 & |V|. V is the set of the corpus vocabulary. Each word is then represent by a V-dimensional binary vector. \n",
    "\n",
    "- One hot encoding is intuitive to undetrstand and straight forward to implement\n",
    "- Size of the one-hot vector is directly proportional to size of the vocabulary. so for large coprora it is computationaly ineffiecient to compute and store.\n",
    "- This doesn't give fixed-length representation.\n",
    "- It treats words as atomic unit and poor at capturing the meaning of the word in relation to other words.(run, ran, apple)\n",
    "- Out of vocabulary problem. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of sentences\n",
    "sent_list = [\"i read newspaper yesterday.\", \"I watched TV Today.\", \"john read newspaper and watched TV today.\"]\n",
    "\n",
    "pre_process_list = [i.lower().replace(\".\",\"\") for i in sent_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i read newspaper yesterday', 'i watched tv today', 'john read newspaper and watched tv today']\n"
     ]
    }
   ],
   "source": [
    "print(pre_process_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary set for the pre-processed list\n",
    "vocab = {}\n",
    "count = 0\n",
    "for i in pre_process_list:\n",
    "    for w in i.split():\n",
    "        if w not in vocab:\n",
    "            count = count + 1\n",
    "            vocab[w] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 1, 'read': 2, 'newspaper': 3, 'yesterday': 4, 'watched': 5, 'tv': 6, 'today': 7, 'john': 8, 'and': 9}\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onehot_encoding(text):\n",
    "    \"\"\"\n",
    "        Generate one hot encoding for string based on vocab set. \n",
    "        If word exisst, it's representation in vocab will be returned.\n",
    "        if not, a list of zero returned.\n",
    "    \"\"\"\n",
    "    one_hot_encoded = []\n",
    "    for w in text.split():\n",
    "        temp = [0]*len(vocab)\n",
    "        if w in vocab:\n",
    "            temp[vocab[w]-1] = 1# -1 because array indexing start from 0\n",
    "        one_hot_encoded.append(temp)\n",
    "    return one_hot_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 1, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_onehot_encoding(pre_process_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Skikit learn\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'read', 'newspaper', 'yesterday', 'i', 'watched', 'tv', 'today', 'john', 'read', 'newspaper', 'and', 'watched', 'tv', 'today']\n"
     ]
    }
   ],
   "source": [
    "nest_list = [i.split() for i in pre_process_list]\n",
    "\n",
    "word_list = [ item for elem in nest_list for item in elem]\n",
    "\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 4 3 8 1 7 6 5 2 4 3 0 7 6 5]\n"
     ]
    }
   ],
   "source": [
    "# Label Encodeing\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded_values = label_encoder.fit_transform(word_list)\n",
    "\n",
    "print(integer_encoded_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding\n",
    "onehot_encoder = OneHotEncoder()\n",
    "onehot_encoded = onehot_encoder.fit_transform(nest_list)\n",
    "print(onehot_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words\n",
    "    \n",
    "    Similar to one-hot encoding, Bag of word maps to unique integer is between 1 & |V|. Each document in the corpus converted into a vector of |V| dimention. where in the ith component of the vector simply the number od times the word w occurs in the document. Each word in the V by thier occurrences count in the document.\n",
    "    \n",
    "    EX: Vocab =  [i =1, read=2, newspaper=3, yesterday=4, today=5, john=6, TV=7, watch=8]\n",
    "        i read newspaper today. = [1,1,1,0,1,0,0,0]\n",
    "        i read newspaper today, i watch tv today = [2,1,1,0,2,0,1,1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With this method, documents having same words will have thier vector epresentation closer to each other in euclidean space.\n",
    "- Fixed length of encoding for any sentence of length\n",
    "\n",
    "- Size of the vector increase == Size of the vocabulary, Restrict by limiting vocabulary\n",
    "- Doesn''t capture similarity between different words.\n",
    "- Doesn't handle out of vocabulary words\n",
    "- Word order information is lost in this representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i read newspaper yesterday', 'i watched tv today', 'john read newspaper and watched tv today']\n"
     ]
    }
   ],
   "source": [
    "print(pre_process_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initalize count vectorrizer\n",
    "count_vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'read': 3, 'newspaper': 2, 'yesterday': 7, 'watched': 6, 'tv': 5, 'today': 4, 'john': 1, 'and': 0}\n"
     ]
    }
   ],
   "source": [
    "# Build BOW for the word list\n",
    "bow = count_vect.fit_transform(pre_process_list)\n",
    "\n",
    "print(count_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i read newspaper yesterday':  [[0 0 1 1 0 0 0 1]]\n",
      "i watched tv today:  [[0 0 0 0 1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"i read newspaper yesterday': \", bow[0].toarray())\n",
    "print(\"i watched tv today: \",bow[1].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i read newspaper today i watch tv today':  [[0 0 1 1 2 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "new_text = count_vect.transform([\"i read newspaper today i watch tv today\"])\n",
    "print(\"i read newspaper today i watch tv today': \", new_text.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i read newspaper today i watch tv today': [[0 0 1 1 1 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#BoW with binary vectors\n",
    "count_vect = CountVectorizer(binary=True)\n",
    "bow_rep_bin = count_vect.fit_transform(pre_process_list)\n",
    "text_2 = count_vect.transform([\"i read newspaper today i watch tv today\"])\n",
    "print(\"i read newspaper today i watch tv today':\", text_2.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of N-Grams\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "## Distributed Representations\n",
    "### Distributional similarity\n",
    "### Distributional hypothesis\n",
    "### Distributional representation\n",
    "### Distributed representation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
