{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Representation\n",
    "\n",
    "Transform a pre-processed text into suitable numerical form and fed into ML algorithm for further process is called feature extraction or text representation.\n",
    "Feature extraction is common step in any ml problem such as image, video, audio. \n",
    "\n",
    " - Images will be transform into matrix representation based on their pixel values.\n",
    " - Video also similar, video is just a collection of frames where each frame is an image.  so the video represent as a sequential collection of matrices. \n",
    " - Audio usually transmit as waves. so represent this mathematically, sampled wave amplitude will be \n",
    "recorded. this will give array representation of the sound waves.\n",
    "\n",
    "Text representation approach classified into 4 categories\n",
    "\n",
    " - Basic vectorization approaches\n",
    " - Distributed representations\n",
    " - Universal language representation\n",
    " - Handcrafted Features\n",
    "\n",
    "##### Text which represent by vectors of numbers is called vector space model. It's simple model used for representing any text blob. It's fundamental to many NLP operations like info-retrieval, scoring the documents etc.,  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic vectorization approaches\n",
    "Match each word in the vocabulary of the text corpus to a unique ID(integer). Then represent sentence in the corpus as a v-dimensional vector. \n",
    "\n",
    "### One- Hot Encoding\n",
    "In this method, each word w in the corpus given a unique integer ID, It's between 1 & |V|. V is the set of the corpus vocabulary. Each word is then represent by a V-dimensional binary vector. \n",
    "\n",
    "- One hot encoding is intuitive to undetrstand and straight forward to implement\n",
    "- Size of the one-hot vector is directly proportional to size of the vocabulary. so for large coprora it is computationaly ineffiecient to compute and store.\n",
    "- This doesn't give fixed-length representation.\n",
    "- It treats words as atomic unit and poor at capturing the meaning of the word in relation to other words.(run, ran, apple)\n",
    "- Out of vocabulary problem. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of sentences\n",
    "sent_list = [\"i read newspaper yesterday.\", \"I watched TV Today.\", \"john read newspaper and watched TV today.\"]\n",
    "\n",
    "pre_process_list = [i.lower().replace(\".\",\"\") for i in sent_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i read newspaper yesterday', 'i watched tv today', 'john read newspaper and watched tv today']\n"
     ]
    }
   ],
   "source": [
    "print(pre_process_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary set for the pre-processed list\n",
    "vocab = {}\n",
    "count = 0\n",
    "for i in pre_process_list:\n",
    "    for w in i.split():\n",
    "        if w not in vocab:\n",
    "            count = count + 1\n",
    "            vocab[w] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 1, 'read': 2, 'newspaper': 3, 'yesterday': 4, 'watched': 5, 'tv': 6, 'today': 7, 'john': 8, 'and': 9}\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onehot_encoding(text):\n",
    "    \"\"\"\n",
    "        Generate one hot encoding for string based on vocab set. \n",
    "        If word exisst, it's representation in vocab will be returned.\n",
    "        if not, a list of zero returned.\n",
    "    \"\"\"\n",
    "    one_hot_encoded = []\n",
    "    for w in text.split():\n",
    "        temp = [0]*len(vocab)\n",
    "        if w in vocab:\n",
    "            temp[vocab[w]-1] = 1# -1 because array indexing start from 0\n",
    "        one_hot_encoded.append(temp)\n",
    "    return one_hot_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 1, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_onehot_encoding(pre_process_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Skikit learn\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'read', 'newspaper', 'yesterday', 'i', 'watched', 'tv', 'today', 'john', 'read', 'newspaper', 'and', 'watched', 'tv', 'today']\n"
     ]
    }
   ],
   "source": [
    "nest_list = [i.split() for i in pre_process_list]\n",
    "\n",
    "word_list = [ item for elem in nest_list for item in elem]\n",
    "\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 4 3 8 1 7 6 5 2 4 3 0 7 6 5]\n"
     ]
    }
   ],
   "source": [
    "# Label Encodeing\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded_values = label_encoder.fit_transform(word_list)\n",
    "\n",
    "print(integer_encoded_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding\n",
    "onehot_encoder = OneHotEncoder()\n",
    "onehot_encoded = onehot_encoder.fit_transform(nest_list)\n",
    "print(onehot_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words\n",
    "    \n",
    "    Similar to one-hot encoding, Bag of word maps to unique integer is between 1 & |V|. Each document in the corpus converted into a vector of |V| dimention. where in the ith component of the vector simply the number od times the word w occurs in the document. Each word in the V by thier occurrences count in the document.\n",
    "    \n",
    "    EX: Vocab =  [i =1, read=2, newspaper=3, yesterday=4, today=5, john=6, TV=7, watch=8]\n",
    "        i read newspaper today. = [1,1,1,0,1,0,0,0]\n",
    "        i read newspaper today, i watch tv today = [2,1,1,0,2,0,1,1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With this method, documents having same words will have thier vector epresentation closer to each other in euclidean space.\n",
    "- Fixed length of encoding for any sentence of length\n",
    "\n",
    "- Size of the vector increase == Size of the vocabulary, Restrict by limiting vocabulary\n",
    "- Doesn''t capture similarity between different words.\n",
    "- Doesn't handle out of vocabulary words\n",
    "- Word order information is lost in this representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i read newspaper yesterday', 'i watched tv today', 'john read newspaper and watched tv today']\n"
     ]
    }
   ],
   "source": [
    "print(pre_process_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initalize count vectorrizer\n",
    "count_vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'read': 3, 'newspaper': 2, 'yesterday': 7, 'watched': 6, 'tv': 5, 'today': 4, 'john': 1, 'and': 0}\n"
     ]
    }
   ],
   "source": [
    "# Build BOW for the word list\n",
    "bow = count_vect.fit_transform(pre_process_list)\n",
    "\n",
    "print(count_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i read newspaper yesterday':  [[0 0 1 1 0 0 0 1]]\n",
      "i watched tv today:  [[0 0 0 0 1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"i read newspaper yesterday': \", bow[0].toarray())\n",
    "print(\"i watched tv today: \",bow[1].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i read newspaper today i watch tv today':  [[0 0 1 1 2 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "new_text = count_vect.transform([\"i read newspaper today i watch tv today\"])\n",
    "print(\"i read newspaper today i watch tv today': \", new_text.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i read newspaper today i watch tv today': [[0 0 1 1 1 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#BoW with binary vectors\n",
    "count_vect = CountVectorizer(binary=True)\n",
    "bow_rep_bin = count_vect.fit_transform(pre_process_list)\n",
    "text_2 = count_vect.transform([\"i read newspaper today i watch tv today\"])\n",
    "print(\"i read newspaper today i watch tv today':\", text_2.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of N-Grams\n",
    "    One-hot encoding and bag of words treat words as independent units. and there is no word ordering. The bag od N-grams tries to solve this by breaking text into chunks of n touching words. This will help to capture context. Each chunk is called n-gram. vocabulary is nothing but collection of all unique n-gram.\n",
    "    \n",
    "    ex: bigram model - i read newspaper today,i watch tv today - i read, read newspaper, newspaper today, today i, i watch, watch tv, tv today.\n",
    "    \n",
    "    - It capture some context and word-order information in the orm of n-grams\n",
    "    - Because of above it can able to capture some semantic similarity. \n",
    "    - As n increases, dimensionality only increases rapidly\n",
    "    - It doen't address OOV problem (handling out of vocabulary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'read': 10, 'newspaper': 6, 'yesterday': 20, 'read newspaper': 11, 'newspaper yesterday': 9, 'read newspaper yesterday': 13, 'watched': 17, 'tv': 15, 'today': 14, 'watched tv': 18, 'tv today': 16, 'watched tv today': 19, 'john': 3, 'and': 0, 'john read': 4, 'newspaper and': 7, 'and watched': 1, 'john read newspaper': 5, 'read newspaper and': 12, 'newspaper and watched': 8, 'and watched tv': 2}\n"
     ]
    }
   ],
   "source": [
    "bow = count_vect.fit_transform(pre_process_list)\n",
    "\n",
    "print(count_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i read newspaper yesterday':  [[0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1]]\n",
      "i watched tv today:  [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"i read newspaper yesterday': \", bow[0].toarray())\n",
    "print(\"i watched tv today: \",bow[1].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i read newspaper today i watch tv today':  [[0 0 0 0 0 0 1 0 0 0 1 1 0 0 2 1 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "new_text = count_vect.transform([\"i read newspaper today i watch tv today\"])\n",
    "print(\"i read newspaper today i watch tv today': \", new_text.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "     In all other approaches text are treated as important. there is no impression of some words in the document being more important than others. TF-IDF (Term Frequenct-inverse document frequency ) solv this problem. It try to quantify the importance of a given word relative to other words. it commonly used in information-retriveal system. \n",
    "     \n",
    "     Idea behind TF-IDF is if the word \"W\" apperas in many times in doument A and not occur much in document B. then word \"W\" is much important to document A.\n",
    "     \n",
    "     TF - measures how often a term or word occurs in given document. This may give biased results when comes to longer documents. to resolve that the number of occurance divided by the length of the doument.\n",
    "     \n",
    "     IDF - Measure the importance of the term across a corpus. when computing TF all terms are given equal importance.but stop words are very common and occur many times in the document and those words are not important. So IDF weighs down the terms that are very common across coprus and weighs up the rare terms.\n",
    "     \n",
    "     IDF = loge(total number of documents in the corpus) (Number of documents with term t in them)\n",
    "     \n",
    "     TF-IDF score  = TF * IDF\n",
    "     \n",
    "     Even though it perform better compare to other methods still suffers from the curs of high dimensionality.\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_tfidf = tfidf.fit_transform(pre_process_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF for all words in the vocabulary [1.69314718 1.69314718 1.28768207 1.28768207 1.28768207 1.28768207\n",
      " 1.28768207 1.69314718]\n",
      "All words in the vocabulary ['and', 'john', 'newspaper', 'read', 'today', 'tv', 'watched', 'yesterday']\n"
     ]
    }
   ],
   "source": [
    "#IDF for all words in the vocabulary\n",
    "print(\"IDF for all words in the vocabulary\",tfidf.idf_)\n",
    "\n",
    "#All words in the vocabulary.\n",
    "print(\"All words in the vocabulary\",tfidf.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF representation for all documents in our corpus\n",
      " [[0.         0.         0.51785612 0.51785612 0.         0.\n",
      "  0.         0.68091856]\n",
      " [0.         0.         0.         0.         0.57735027 0.57735027\n",
      "  0.57735027 0.        ]\n",
      " [0.45212331 0.45212331 0.34385143 0.34385143 0.34385143 0.34385143\n",
      "  0.34385143 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"TFIDF representation for all documents in our corpus\\n\",bow_tfidf.toarray()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i read newspaper today i watch tv today':  [[0.         0.         0.37796447 0.37796447 0.75592895 0.37796447\n",
      "  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "new_text = tfidf.transform([\"i read newspaper today i watch tv today\"])\n",
    "print(\"i read newspaper today i watch tv today': \", new_text.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Representations\n",
    "    There are some key drawbacks in the vectorization method. To over come that method to learn low-dimentional representation were devised. \n",
    "    they use nural netwrok architecture to create dense, low dimnetional representation of words and texts. \n",
    "    \n",
    "### Distributional similarity\n",
    "    This is called meaning understood by context. ex: \"MJ Rocks\" - Rocks literally meaning stones but in this context it means good.\n",
    "    \n",
    "### Distributional hypothesis\n",
    "    Word that occur in similar context have similar meanings. cat, mouse. is having similar context that's animal and their characterstics. according to distribuationl hypothesis there should be strong similarities between the meaning of these two words.\n",
    "    \n",
    "### Distributional representation\n",
    "    \n",
    "    \n",
    "### Distributed representation\n",
    "\n",
    "\n",
    "### Embedding\n",
    "    Embedding is a mapping between vector space coming from distibutional representation to vector space coming from distributed representation.\n",
    "\n",
    "### Vector semantics\n",
    "    Set of NLP methods that aim to learn the word representations based on distributional properties words in a large corpus.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embessings\n",
    "    Lets just say we give word \"Cat\" distributionally similar words could be other animals either domostic or wild animals.  Neural network word2vec model based on distributional similarity can capture word analogy relationship \"king-man+woman = queen\". when we learn semantically rich relationships word2vec ensures that the learned word representation are low dimentional.Thease representaion are called embeddings.\n",
    "    \n",
    "    Conceptually, Word2vec takes a large corpus of text as input and “learns” to represent the words in a common vector space based on the contexts in which they appear in the corpus. Given a word w and the words appearing in its context C, how do we find the vector that best represents the meaning of the word? For every word w in corpus, we start with a vector v initialized with random values. The Word2vec model refines the values in v by predicting v , given the vectors for words in the context C. It does this using a two-layer neural network.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Trained Word embeddings\n",
    "    Train our own embedding is pretty expensive in both time and computing. Some one already trained embedding on large cporpus such as  wikipedia, news article or entier web. these contain key value pair where key represent the words and value represents corresponding word vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings #This module ignores the various types of warnings generated\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "import os #This module provides a way of using operating system dependent functionality\n",
    "\n",
    "import psutil #This module helps in retrieving information on running processes and system resource utilization\n",
    "process = psutil.Process(os.getpid())\n",
    "from psutil import virtual_memory\n",
    "mem = virtual_memory()\n",
    "\n",
    "import time #This module is used to calculate the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained_embeddings = \"../data/GoogleNews-vectors-negative300.bin.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.094830592\n"
     ]
    }
   ],
   "source": [
    "print(process.memory_info().rss / 10**9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total_memory_available:8424902656\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total_memory_available:{mem.total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = KeyedVectors.load_word2vec_format(pre_trained_embeddings, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.559550976\n"
     ]
    }
   ],
   "source": [
    "print(process.memory_info().rss / 10**9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocablulary:  3000000\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of words in vocablulary: \",len(w2v_model.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cats', 0.8099379539489746),\n",
       " ('dog', 0.7609456777572632),\n",
       " ('kitten', 0.7464985251426697),\n",
       " ('feline', 0.7326233983039856),\n",
       " ('beagle', 0.7150583267211914),\n",
       " ('puppy', 0.7075453996658325),\n",
       " ('pup', 0.6934291124343872),\n",
       " ('pet', 0.6891531348228455),\n",
       " ('felines', 0.6755931377410889),\n",
       " ('chihuahua', 0.6709762215614319)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check most similar words\n",
    "w2v_model.most_similar('cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('telephone', 0.8224020600318909),\n",
       " ('cell_phone', 0.7831966876983643),\n",
       " ('cellphone', 0.7629485130310059),\n",
       " ('Phone', 0.7060797214508057),\n",
       " ('phones', 0.6894922256469727),\n",
       " ('landline', 0.6263927221298218),\n",
       " ('voicemail', 0.6252243518829346),\n",
       " ('caller_id', 0.6023746132850647),\n",
       " ('RingCentral_cloud_computing', 0.5935890674591064),\n",
       " ('telephones', 0.5929964780807495)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check most similar words\n",
    "w2v_model.most_similar('phone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.81250000e-03,  2.05078125e-02,  1.89453125e-01,  2.85156250e-01,\n",
       "       -2.55859375e-01,  4.61425781e-02,  2.36816406e-02, -1.11328125e-01,\n",
       "        1.50390625e-01,  3.83300781e-02, -1.41601562e-02, -3.65234375e-01,\n",
       "       -7.56835938e-02,  2.09960938e-02, -1.19140625e-01,  1.63085938e-01,\n",
       "        1.05468750e-01,  1.64062500e-01, -2.03857422e-02, -6.64062500e-02,\n",
       "        7.95898438e-02,  1.75781250e-01,  1.32812500e-01,  9.42382812e-02,\n",
       "        3.44238281e-02, -1.90429688e-01,  1.40625000e-01,  1.60156250e-01,\n",
       "       -5.10253906e-02, -3.54003906e-03, -1.42578125e-01,  1.19140625e-01,\n",
       "       -3.49121094e-02, -1.82617188e-01,  1.10839844e-01, -1.82617188e-01,\n",
       "       -8.54492188e-02, -1.46484375e-01, -3.24707031e-02,  7.17773438e-02,\n",
       "        7.51953125e-02, -3.58886719e-02,  2.20703125e-01, -1.56250000e-01,\n",
       "       -2.39257812e-01,  1.47460938e-01, -3.61328125e-02, -4.10156250e-02,\n",
       "        7.53784180e-03, -2.87109375e-01, -1.25000000e-01, -7.56835938e-02,\n",
       "        2.15820312e-01,  1.04492188e-01,  8.59375000e-02, -2.55859375e-01,\n",
       "       -1.76757812e-01, -1.08886719e-01,  1.23046875e-01,  1.76757812e-01,\n",
       "       -1.34765625e-01, -1.21093750e-01, -2.17285156e-02,  4.29687500e-02,\n",
       "        4.22363281e-02,  1.79687500e-01, -6.59179688e-02,  8.05664062e-02,\n",
       "        2.69531250e-01, -1.04003906e-01,  8.05664062e-02,  1.41601562e-01,\n",
       "        3.49609375e-01, -1.29882812e-01,  1.10351562e-01, -1.56250000e-01,\n",
       "       -8.05664062e-02, -4.80957031e-02,  3.64780426e-05, -5.37109375e-02,\n",
       "        1.17675781e-01, -6.83593750e-03,  1.11816406e-01,  1.66015625e-02,\n",
       "       -8.85009766e-03, -2.22167969e-02, -8.25195312e-02,  7.32421875e-02,\n",
       "       -2.78320312e-02,  1.65039062e-01, -7.37304688e-02,  1.30859375e-01,\n",
       "        3.97949219e-02,  3.11279297e-02, -1.44531250e-01, -2.19345093e-04,\n",
       "        7.42187500e-02,  1.11816406e-01, -1.40380859e-02,  1.26953125e-02,\n",
       "       -5.92041016e-03, -2.47802734e-02,  3.41796875e-01, -1.25122070e-02,\n",
       "       -1.75781250e-02, -1.24511719e-01,  9.76562500e-02, -2.63671875e-01,\n",
       "       -5.02929688e-02, -6.68945312e-02,  1.26953125e-01,  1.74804688e-01,\n",
       "       -7.61718750e-02, -1.72851562e-01,  9.13085938e-02, -1.92382812e-01,\n",
       "       -4.22363281e-02,  3.27148438e-02, -4.39453125e-02,  6.29425049e-04,\n",
       "       -8.98437500e-02,  7.42187500e-02, -9.22851562e-02,  1.58203125e-01,\n",
       "       -6.43920898e-03, -1.57226562e-01, -1.32812500e-01, -4.17480469e-02,\n",
       "       -3.58886719e-02,  2.57812500e-01, -5.29785156e-02, -6.53076172e-03,\n",
       "        8.54492188e-02,  1.57226562e-01,  3.14941406e-02,  2.31933594e-02,\n",
       "       -7.51953125e-02,  8.34960938e-02, -8.34960938e-02, -5.59082031e-02,\n",
       "        2.91015625e-01, -1.20117188e-01,  5.05371094e-02, -1.13769531e-01,\n",
       "        4.56542969e-02,  1.35742188e-01,  3.35937500e-01, -1.53320312e-01,\n",
       "       -7.81250000e-02,  4.51660156e-02,  1.45874023e-02,  2.87109375e-01,\n",
       "       -1.20117188e-01, -4.54101562e-02, -2.43164062e-01,  4.02832031e-02,\n",
       "        1.09375000e-01, -6.25000000e-02,  5.78613281e-02, -2.19726562e-02,\n",
       "        1.40625000e-01,  9.08203125e-02, -9.03320312e-02,  2.28515625e-01,\n",
       "        2.50000000e-01, -3.83300781e-02,  2.96875000e-01, -4.24194336e-03,\n",
       "        1.91406250e-01,  6.44531250e-02, -3.39843750e-01,  3.93066406e-02,\n",
       "       -5.83496094e-02,  1.28906250e-01, -3.83300781e-02,  5.15136719e-02,\n",
       "        9.27734375e-02, -2.12890625e-01, -3.12500000e-02,  2.73437500e-02,\n",
       "        8.54492188e-03, -2.67578125e-01,  2.51953125e-01, -7.12890625e-02,\n",
       "       -1.70898438e-01,  1.36108398e-02,  6.83593750e-02, -9.52148438e-02,\n",
       "        1.03515625e-01, -1.86523438e-01,  2.22656250e-01,  7.66601562e-02,\n",
       "       -6.83593750e-02, -1.28906250e-01, -6.50024414e-03,  1.89453125e-01,\n",
       "        8.25195312e-02, -1.26953125e-02, -1.69921875e-01, -1.16210938e-01,\n",
       "        3.51562500e-02,  1.80664062e-01, -3.47656250e-01,  4.46777344e-02,\n",
       "        1.34765625e-01, -1.97265625e-01,  3.90625000e-02,  1.00097656e-01,\n",
       "        1.04980469e-01, -2.06298828e-02, -2.68554688e-02,  3.96484375e-01,\n",
       "       -1.40625000e-01, -1.66992188e-01, -3.10546875e-01, -5.76171875e-02,\n",
       "        2.70080566e-03, -1.25976562e-01, -3.32031250e-01, -3.02734375e-01,\n",
       "        6.88476562e-02,  5.46875000e-02, -1.24023438e-01,  1.50390625e-01,\n",
       "       -1.94335938e-01, -1.61132812e-01,  2.48046875e-01,  1.82617188e-01,\n",
       "        2.40478516e-02, -2.79296875e-01,  4.37011719e-02, -2.19726562e-01,\n",
       "        1.52343750e-01, -1.42578125e-01,  2.89062500e-01, -3.68652344e-02,\n",
       "        8.64257812e-02, -1.54296875e-01,  2.50000000e-01, -1.74560547e-02,\n",
       "       -7.27539062e-02,  5.71289062e-02,  2.34375000e-02, -7.86132812e-02,\n",
       "       -1.39648438e-01,  1.16699219e-01,  1.35742188e-01, -5.81054688e-02,\n",
       "       -1.00097656e-01, -1.66992188e-01,  8.59375000e-02,  9.13085938e-02,\n",
       "        2.27539062e-01, -6.34765625e-02, -2.34375000e-01,  1.28906250e-01,\n",
       "       -1.37695312e-01,  1.24023438e-01, -2.59765625e-01, -9.57031250e-02,\n",
       "       -1.46484375e-01,  1.53320312e-01,  6.20117188e-02, -4.30297852e-03,\n",
       "        8.69140625e-02,  3.28125000e-01, -5.24902344e-02,  6.59179688e-02,\n",
       "       -2.24609375e-02,  1.87500000e-01,  7.03125000e-02,  5.15625000e-01,\n",
       "        1.80664062e-01,  1.05468750e-01,  2.67578125e-01, -3.51562500e-01,\n",
       "       -5.44433594e-02, -1.56250000e-01,  1.17187500e-02, -8.74023438e-02,\n",
       "        4.66308594e-02,  9.81445312e-02, -1.79687500e-01, -8.34960938e-02,\n",
       "       -1.83593750e-01, -9.39941406e-03, -1.78710938e-01,  1.78710938e-01,\n",
       "        1.30859375e-01, -6.44531250e-02,  1.85546875e-01,  6.83593750e-02,\n",
       "       -9.76562500e-02,  8.54492188e-02,  1.03027344e-01, -3.18359375e-01,\n",
       "       -7.91015625e-02,  2.38037109e-02,  6.07910156e-02, -1.21582031e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find vector representation for the word\n",
    "w2v_model['watch']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training own embeddings\n",
    "    * Continous bag of words\n",
    "    * Skipgram\n",
    "\n",
    "### Continuous bag of words\n",
    "    Primary task is to build a model that predicts center word give the context word in which the center word appears. language model is statistical model that tries to gie probabilty distribution to the sequence of words. objective of the language model is to give high probability to good sentences and low probability to bad sentences. Good sentences means the sentences which is semantically and syntactically correct. ex: \"cat jumped over the dog \" prob = 1.0 . \"jumped over the dog cat\" prob=0.0\n",
    "    \n",
    "    CBOW tries to learn a model that predict the center word from in its context. ex : The furious tiger killed many people. center word is targer and remain words in the windows are y.\n",
    "    \n",
    "    if k =2 ie context = 2/ use 2k+1 sliding window to find the context and target. \n",
    "    The furious tiger = the, furious)tiger\n",
    "    The furious tiger killed = the, tiger, kiled) furious\n",
    "\n",
    "### SkipGram\n",
    "    it's similar to CBOW with some minor changes.in 2k+1 sliding window unlike cbow center words in the window is X, and k words on either side of the center word is y. \n",
    "    \n",
    "    The furious tiger = (the, furious)(the, tiger)\n",
    "    The furious tiger killed = (furious, the)(furious, tiger)(furious, killed)\n",
    "    \n",
    "    there are several hyper parameter tuning available like window size, dimentionality of the vectorss to be learned, learin rate, epoche.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import test data from gensim model\n",
    "from gensim.test.utils import common_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'], ['survey', 'user', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'system'], ['system', 'human', 'system', 'eps'], ['user', 'response', 'time'], ['trees'], ['graph', 'trees'], ['graph', 'minors', 'trees'], ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "print(common_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(common_texts, size=8, window=4, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../data/gensim_common_text_model.w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('human', 0.3920215368270874), ('minors', 0.24046307802200317), ('user', 0.23201751708984375), ('interface', 0.197819322347641), ('survey', 0.1638130098581314), ('eps', -0.008773356676101685), ('trees', -0.013679832220077515), ('time', -0.13756054639816284), ('graph', -0.1475311517715454), ('response', -0.2668741047382355)]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(\"computer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0039434   0.04938755 -0.05820011  0.02601842 -0.00767939  0.05581497\n",
      " -0.01596179 -0.01445093]\n"
     ]
    }
   ],
   "source": [
    "print(model[\"computer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embedding to get feature represerntation of larger text.\n",
    "    Simple approach is to break sentence into words then take embedding for individual words then combin everything together.\n",
    "    \n",
    "    Both pre-trained and self-trained word embeddings spend on the vocabulary they see it on the training data. W2v or any other text representation don't have good way of handling out of vocabulary words. simple approach is to exclude thse words from the feature extraction process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nlp(\"goa is wonderful tourist destination\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.89549    0.38773    0.64984   -0.16708    0.72494   -0.065563\n",
      "  0.20031   -0.39032    0.35382    0.74078   -1.3711    -0.65238\n",
      " -0.38228   -0.23277    0.47455    0.14023   -0.27709    1.9277\n",
      "  0.55714    0.76838    0.24489    0.091607   0.15209    0.087329\n",
      "  0.19561    0.070279   0.17415    0.019008  -0.35183    1.146\n",
      "  0.28155   -0.82137    0.099048   0.25678   -0.42638    0.27792\n",
      " -0.25204   -0.31803   -0.50234   -0.36031    0.13668   -0.70532\n",
      "  0.2811    -0.56934   -0.40299   -0.51336    0.17735   -0.18854\n",
      "  0.50197    0.15772   -0.036079  -0.066684  -0.25667   -0.81924\n",
      " -0.28292    0.32283   -0.041018  -0.42019   -0.018701  -0.46989\n",
      " -0.57918   -0.57153    0.19196   -0.33212    0.19154   -0.075422\n",
      " -0.015175   0.63033    0.10762   -0.14905   -0.10694   -0.024239\n",
      " -0.13572   -0.29651    0.72742    0.1151    -0.18163   -0.10087\n",
      "  0.97217   -0.058608   0.33354   -0.016199  -0.3009     0.51322\n",
      " -0.45041   -0.40163    0.6992     0.45031    0.56161   -0.19313\n",
      " -0.086237   0.13358   -0.67141   -0.37644    0.2718     0.14765\n",
      "  0.62897   -0.24103   -0.29991    0.2174     0.2567     0.2449\n",
      "  0.55241    0.16982    0.2599    -1.3262    -0.3595     0.60614\n",
      "  0.27597    0.13277   -0.2544    -0.40686   -0.18221   -0.56993\n",
      " -0.12814   -0.10779    0.73404    0.44591   -0.067276  -0.14073\n",
      " -0.10231    0.11481    0.27727   -0.22383   -0.49846    0.63419\n",
      "  0.66321    0.15422    0.050868  -0.25247    0.39205   -0.93202\n",
      " -0.55582   -0.39244    0.53452   -0.34011   -0.096154  -0.025587\n",
      " -0.04725   -0.06002   -1.4269    -0.15018    0.32504   -0.32352\n",
      " -0.058275   0.0893     0.47223    0.29497   -0.307      0.38071\n",
      "  0.43707    0.22544   -0.061063  -0.22219    0.32978    0.15271\n",
      "  0.19613    0.58791    0.15562    0.79809    0.90413    0.058271\n",
      "  0.69335   -0.04934    0.0042887 -0.21635   -0.38752   -0.29472\n",
      " -0.052688   0.079672  -0.34789   -0.6912    -0.2438    -0.020453\n",
      "  0.022781  -0.047224  -0.065761   0.67075    0.57824   -0.15308\n",
      "  0.096082   0.18484    0.2547    -0.1917     0.9336    -0.17869\n",
      " -0.063854  -0.36564    0.88375    0.47959   -0.11823   -0.35545\n",
      "  0.1105    -0.59521   -0.7309    -0.3806     0.69373    0.49869\n",
      "  0.081713  -0.44819   -0.0414    -0.079019   0.073122  -0.08452\n",
      " -0.20756   -0.27516    0.48483   -0.2088     0.11127    0.45291\n",
      " -0.6464    -0.088146  -0.033981   0.12622   -0.24043   -0.85152\n",
      " -0.71607   -0.15317   -0.12615    0.021336  -0.50427    0.2285\n",
      "  0.29442    0.10858    0.042304   0.45134   -0.36161    0.40047\n",
      " -0.36279   -0.45661    0.11676    0.36681   -0.17286   -0.33635\n",
      " -0.45821   -0.79517    0.12095    0.034343   0.01474    0.22938\n",
      " -0.37771    0.17575    0.046418  -0.33897    0.59553   -0.31441\n",
      "  0.28463    0.081307  -0.34542   -0.63689    0.25396    0.038911\n",
      " -0.17136   -0.59213    0.43145    0.12537   -0.02965    0.3441\n",
      " -0.5956    -0.31397    0.76057   -0.19094   -0.058879   0.46321\n",
      " -0.22074   -0.64109    0.033582  -0.051244   0.22457    0.67154\n",
      " -0.69952    0.14563   -0.2467     1.2914     0.32973    0.48938\n",
      " -0.1946    -0.34252    0.24076    0.17426   -0.17678   -0.14897\n",
      "  0.090902   0.60383   -0.22937    0.15946    0.28343   -0.46851\n",
      "  0.15138   -0.14781    0.094661  -0.23491   -0.22557    0.05284\n",
      " -0.25437    0.079517  -0.12207    0.1375    -0.26706    0.57569  ]\n"
     ]
    }
   ],
   "source": [
    "# vector for individual word\n",
    "print(text[0].vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.45461798e-01  1.82797998e-01  2.62645492e-03 -2.22708389e-01\n",
      "  6.11549973e-01  1.53727397e-01  1.23977400e-01 -2.01021999e-01\n",
      " -1.79411396e-01  2.07283592e+00 -5.95480978e-01 -1.43926412e-01\n",
      " -1.35687992e-01  2.26209946e-02  1.33888215e-01 -3.11084002e-01\n",
      " -2.24286795e-01  1.32608414e+00  4.16280985e-01  9.98557955e-02\n",
      " -1.20672002e-01  3.47440019e-02 -9.54186022e-02  1.35446265e-01\n",
      "  2.04374820e-01 -1.16621945e-02  8.05706009e-02 -1.45420805e-01\n",
      " -4.11785990e-01  4.81037915e-01  1.60996795e-01 -3.44648600e-01\n",
      "  1.58593610e-01  1.81034595e-01 -1.62568256e-01  4.78303954e-02\n",
      "  1.60867810e-01 -2.12299988e-01 -1.19415268e-01 -1.19790994e-01\n",
      "  1.66080091e-02 -1.35701984e-01 -1.15529932e-02 -1.90399989e-01\n",
      " -9.51439962e-02 -9.52480547e-03  1.07617997e-01 -1.02274001e-01\n",
      "  1.48561209e-01 -6.36880025e-02 -5.66932037e-02 -1.40958009e-02\n",
      " -2.96495967e-02 -1.82335183e-01  2.02155992e-01  7.04765990e-02\n",
      "  8.09641927e-02 -8.16802010e-02  2.17477977e-02  5.74860089e-02\n",
      "  3.32629686e-04 -3.56807381e-01 -2.63027996e-01 -6.54624030e-02\n",
      " -1.69979796e-01 -8.76529887e-02 -1.14308991e-01  2.64361560e-01\n",
      "  7.06768036e-02  6.15604036e-02  4.39179949e-02 -4.50519994e-02\n",
      "  4.29059844e-03 -4.20376062e-02  9.14315954e-02  6.89043999e-02\n",
      " -2.04357788e-01  3.08840033e-02 -1.51080014e-02  6.83881938e-02\n",
      "  3.46848011e-01  8.63220077e-03 -5.84900007e-02  1.91109389e-01\n",
      "  3.50135937e-02 -1.44900084e-02 -2.63234198e-01  3.62477988e-01\n",
      "  2.59791791e-01 -2.58395970e-01 -1.34999394e-01  3.92038018e-01\n",
      " -1.95294008e-01 -1.60567984e-01  2.43606403e-01  5.91593981e-02\n",
      " -2.28984997e-01 -1.66590795e-01  8.48193988e-02  3.20515007e-01\n",
      "  4.98176031e-02 -2.23055989e-01 -4.56245989e-02 -4.75999992e-03\n",
      "  2.47754186e-01 -7.97092021e-01 -3.56194389e-04  6.87070787e-02\n",
      "  1.52348012e-01 -7.53600290e-03 -5.61180003e-02 -2.10725188e-01\n",
      " -1.39241204e-01  7.39914030e-02 -4.00040001e-02 -2.39508003e-01\n",
      "  5.41200042e-02  3.18496019e-01 -3.26933205e-01 -9.52124596e-02\n",
      " -1.39625996e-01  2.28644207e-01 -7.35260025e-02 -1.94271192e-01\n",
      " -1.51975200e-01  1.67263031e-01  4.10959944e-02 -2.25627989e-01\n",
      "  1.02815792e-01 -1.11751996e-01  3.97762731e-02 -3.43791187e-01\n",
      " -1.89315796e-01 -1.62939802e-01 -3.86380032e-02 -2.54148006e-01\n",
      " -8.70845988e-02 -1.45437211e-01 -2.40620086e-03 -9.03780013e-02\n",
      " -1.23970008e+00  1.57734200e-01  1.81037799e-01  6.87560160e-03\n",
      " -1.12291001e-01 -2.80764014e-01  8.36419985e-02 -8.31819996e-02\n",
      "  1.71207920e-01  4.97139990e-02  5.63620031e-02  1.72639593e-01\n",
      "  4.53704000e-02 -2.17186600e-01  1.09105133e-01 -2.06719642e-03\n",
      "  1.96747199e-01  1.46632001e-01  1.13099935e-02  9.06954035e-02\n",
      "  3.91197175e-01  1.02106407e-01  2.77449399e-01  1.22670390e-01\n",
      "  2.54860252e-01 -5.59467971e-02 -9.36853960e-02 -1.33107990e-01\n",
      "  1.59501821e-01 -1.09541984e-02  8.08245987e-02 -9.66549963e-02\n",
      " -3.87339927e-02 -1.72380172e-03  1.32715944e-02  1.51036799e-01\n",
      " -4.60959189e-02 -2.49880001e-01 -3.65660079e-02  2.79705971e-02\n",
      "  2.27480028e-02  1.15039945e-02 -4.94922027e-02 -1.78943396e-01\n",
      "  2.20701605e-01 -2.52916217e-01 -2.01711971e-02  1.17186010e-02\n",
      "  1.49433807e-01  6.85682073e-02 -1.07368015e-01  9.02819633e-03\n",
      " -1.88860055e-02 -4.11042050e-02 -1.68656796e-01 -2.98748016e-01\n",
      "  2.44731396e-01  1.94186028e-02  2.55206585e-01  6.31767958e-02\n",
      " -2.23508000e-01 -7.71346241e-02 -1.70003206e-01  2.10673995e-02\n",
      "  1.37363404e-01 -8.64160247e-03  2.04795003e-01  1.12931989e-01\n",
      " -4.41302024e-02  9.35073942e-02 -1.83902189e-01 -3.88015419e-01\n",
      " -6.11628070e-02 -8.19915980e-02  2.00980008e-02 -7.98164010e-02\n",
      "  1.70399882e-02 -2.37351991e-02 -3.00760008e-02 -7.74483904e-02\n",
      " -2.68739998e-01  1.30319998e-01  3.75716001e-01  1.71752006e-01\n",
      "  1.14086792e-01 -9.28364098e-02 -6.73099980e-02  4.68228012e-01\n",
      " -2.77318805e-01 -1.22859225e-01  2.49114190e-03  1.68772206e-01\n",
      "  1.07920006e-01 -1.03874400e-01  5.52814081e-02 -1.32562011e-01\n",
      "  5.91377914e-02 -2.04707384e-01 -3.09969991e-01  3.48310590e-01\n",
      " -1.56624809e-01  1.35669619e-01 -9.68926027e-02  7.41120055e-02\n",
      "  1.40230983e-01  1.09559987e-02 -1.07021198e-01 -7.16659427e-03\n",
      "  7.83506036e-02 -4.73240376e-01  7.24242032e-02 -8.26034024e-02\n",
      "  9.16519985e-02 -1.58263415e-01 -3.41440029e-02  4.85525787e-01\n",
      "  2.83822000e-01 -2.19578415e-01 -1.90219935e-02  8.08364004e-02\n",
      "  2.10104018e-01 -1.98590785e-01  8.52916017e-02  2.84338053e-02\n",
      "  7.02157989e-02 -2.21291140e-01  1.36383399e-01 -8.12483951e-02\n",
      "  3.46799865e-02  2.03180224e-01 -7.84016609e-01 -3.84683982e-02\n",
      " -2.06721812e-01  2.37519741e-02  1.24421194e-01  2.59811789e-01\n",
      "  7.51139969e-02 -5.60183898e-02  1.83470011e-01  1.59885794e-01\n",
      "  6.11960180e-02 -1.94910794e-01  1.38322428e-01  2.79012382e-01\n",
      "  4.43463996e-02 -1.52056009e-01  1.23013183e-01 -1.07974604e-01\n",
      "  1.66953593e-01  1.29068002e-01 -6.76586032e-02 -1.51302014e-02\n",
      " -1.30655199e-01 -2.49479408e-03  2.74450004e-01 -2.81610005e-02\n",
      " -3.84859927e-02 -2.58799945e-03 -4.03179899e-02  2.65253335e-01]\n"
     ]
    }
   ],
   "source": [
    "print(text.vector) # average vector for the sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* All text representation are biased based on what they saw in training data. \n",
    "* Inlike basic vectoriztion approaches, pre trained embeddings are generally large-sized files. this possese some challenge when deployment\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
