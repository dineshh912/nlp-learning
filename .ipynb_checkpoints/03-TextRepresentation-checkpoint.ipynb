{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Representation\n",
    "\n",
    "Transform a pre-processed text into suitable numerical form and fed into ML algorithm for further process is called feature extraction or text representation.\n",
    "Feature extraction is common step in any ml problem such as image, video, audio. \n",
    "\n",
    " - Images will be transform into matrix representation based on their pixel values.\n",
    " - Video also similar, video is just a collection of frames where each frame is an image.  so the video represent as a sequential collection of matrices. \n",
    " - Audio usually transmit as waves. so represent this mathematically, sampled wave amplitude will be \n",
    "recorded. this will give array representation of the sound waves.\n",
    "\n",
    "Text representation approach classified into 4 categories\n",
    "\n",
    " - Basic vectorization approaches\n",
    " - Distributed representations\n",
    " - Universal language representation\n",
    " - Handcrafted Features\n",
    "\n",
    "##### Text which represent by vectors of numbers is called vector space model. It's simple model used for representing any text blob. It's fundamental to many NLP operations like info-retrieval, scoring the documents etc.,  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic vectorization approaches\n",
    "Match each word in the vocabulary of the text corpus to a unique ID(integer). Then represent sentence in the corpus as a v-dimensional vector. \n",
    "\n",
    "### One- Hot Encoding\n",
    "In this method, each word w in the corpus given a unique integer ID, It's between 1 & |V|. V is the set of the corpus vocabulary. Each word is then represent by a V-dimensional binary vector. \n",
    "\n",
    "- One hot encoding is intuitive to undetrstand and straight forward to implement\n",
    "- Size of the one-hot vector is directly proportional to size of the vocabulary. so for large coprora it is computationaly ineffiecient to compute and store.\n",
    "- This doesn't give fixed-length representation.\n",
    "- It treats words as atomic unit and poor at capturing the meaning of the word in relation to other words.(run, ran, apple)\n",
    "- Out of vocabulary problem. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of sentences\n",
    "sent_list = [\"i read newspaper yesterday.\", \"I watched TV Today.\", \"john read newspaper and watched TV today.\"]\n",
    "\n",
    "pre_process_list = [i.lower().replace(\".\",\"\") for i in sent_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i read newspaper yesterday', 'i watched tv today', 'john read newspaper and watched tv today']\n"
     ]
    }
   ],
   "source": [
    "print(pre_process_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary set for the pre-processed list\n",
    "vocab = {}\n",
    "count = 0\n",
    "for i in pre_process_list:\n",
    "    for w in i.split():\n",
    "        if w not in vocab:\n",
    "            count = count + 1\n",
    "            vocab[w] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'i': 1, 'read': 2, 'newspaper': 3, 'yesterday': 4, 'watched': 5, 'tv': 6, 'today': 7, 'john': 8, 'and': 9}\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_onehot_encoding(text):\n",
    "    \"\"\"\n",
    "        Generate one hot encoding for string based on vocab set. \n",
    "        If word exisst, it's representation in vocab will be returned.\n",
    "        if not, a list of zero returned.\n",
    "    \"\"\"\n",
    "    one_hot_encoded = []\n",
    "    for w in text.split():\n",
    "        temp = [0]*len(vocab)\n",
    "        if w in vocab:\n",
    "            temp[vocab[w]-1] = 1# -1 because array indexing start from 0\n",
    "        one_hot_encoded.append(temp)\n",
    "    return one_hot_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 1, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_onehot_encoding(pre_process_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Skikit learn\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'read', 'newspaper', 'yesterday', 'i', 'watched', 'tv', 'today', 'john', 'read', 'newspaper', 'and', 'watched', 'tv', 'today']\n"
     ]
    }
   ],
   "source": [
    "nest_list = [i.split() for i in pre_process_list]\n",
    "\n",
    "word_list = [ item for elem in nest_list for item in elem]\n",
    "\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 4 3 8 1 7 6 5 2 4 3 0 7 6 5]\n"
     ]
    }
   ],
   "source": [
    "# Label Encodeing\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded_values = label_encoder.fit_transform(word_list)\n",
    "\n",
    "print(integer_encoded_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding\n",
    "onehot_encoder = OneHotEncoder()\n",
    "onehot_encoded = onehot_encoder.fit_transform(nest_list)\n",
    "print(onehot_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words\n",
    "    \n",
    "    Similar to one-hot encoding, Bag of word maps to unique integer is between 1 & |V|. Each document in the corpus converted into a vector of |V| dimention. where in the ith component of the vector simply the number od times the word w occurs in the document. Each word in the V by thier occurrences count in the document.\n",
    "    \n",
    "    EX: Vocab =  [i =1, read=2, newspaper=3, yesterday=4, today=5, john=6, TV=7, watch=8]\n",
    "        i read newspaper today. = [1,1,1,0,1,0,0,0]\n",
    "        i read newspaper today, i watch tv today = [2,1,1,0,2,0,1,1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- With this method, documents having same words will have thier vector epresentation closer to each other in euclidean space.\n",
    "- Fixed length of encoding for any sentence of length\n",
    "\n",
    "- Size of the vector increase == Size of the vocabulary, Restrict by limiting vocabulary\n",
    "- Doesn''t capture similarity between different words.\n",
    "- Doesn't handle out of vocabulary words\n",
    "- Word order information is lost in this representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i read newspaper yesterday', 'i watched tv today', 'john read newspaper and watched tv today']\n"
     ]
    }
   ],
   "source": [
    "print(pre_process_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initalize count vectorrizer\n",
    "count_vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'read': 3, 'newspaper': 2, 'yesterday': 7, 'watched': 6, 'tv': 5, 'today': 4, 'john': 1, 'and': 0}\n"
     ]
    }
   ],
   "source": [
    "# Build BOW for the word list\n",
    "bow = count_vect.fit_transform(pre_process_list)\n",
    "\n",
    "print(count_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i read newspaper yesterday':  [[0 0 1 1 0 0 0 1]]\n",
      "i watched tv today:  [[0 0 0 0 1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"i read newspaper yesterday': \", bow[0].toarray())\n",
    "print(\"i watched tv today: \",bow[1].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i read newspaper today i watch tv today':  [[0 0 1 1 2 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "new_text = count_vect.transform([\"i read newspaper today i watch tv today\"])\n",
    "print(\"i read newspaper today i watch tv today': \", new_text.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i read newspaper today i watch tv today': [[0 0 1 1 1 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "#BoW with binary vectors\n",
    "count_vect = CountVectorizer(binary=True)\n",
    "bow_rep_bin = count_vect.fit_transform(pre_process_list)\n",
    "text_2 = count_vect.transform([\"i read newspaper today i watch tv today\"])\n",
    "print(\"i read newspaper today i watch tv today':\", text_2.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of N-Grams\n",
    "    One-hot encoding and bag of words treat words as independent units. and there is no word ordering. The bag od N-grams tries to solve this by breaking text into chunks of n touching words. This will help to capture context. Each chunk is called n-gram. vocabulary is nothing but collection of all unique n-gram.\n",
    "    \n",
    "    ex: bigram model - i read newspaper today,i watch tv today - i read, read newspaper, newspaper today, today i, i watch, watch tv, tv today.\n",
    "    \n",
    "    - It capture some context and word-order information in the orm of n-grams\n",
    "    - Because of above it can able to capture some semantic similarity. \n",
    "    - As n increases, dimensionality only increases rapidly\n",
    "    - It doen't address OOV problem (handling out of vocabulary)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'read': 10, 'newspaper': 6, 'yesterday': 20, 'read newspaper': 11, 'newspaper yesterday': 9, 'read newspaper yesterday': 13, 'watched': 17, 'tv': 15, 'today': 14, 'watched tv': 18, 'tv today': 16, 'watched tv today': 19, 'john': 3, 'and': 0, 'john read': 4, 'newspaper and': 7, 'and watched': 1, 'john read newspaper': 5, 'read newspaper and': 12, 'newspaper and watched': 8, 'and watched tv': 2}\n"
     ]
    }
   ],
   "source": [
    "bow = count_vect.fit_transform(pre_process_list)\n",
    "\n",
    "print(count_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i read newspaper yesterday':  [[0 0 0 0 0 0 1 0 0 1 1 1 0 1 0 0 0 0 0 0 1]]\n",
      "i watched tv today:  [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"i read newspaper yesterday': \", bow[0].toarray())\n",
    "print(\"i watched tv today: \",bow[1].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i read newspaper today i watch tv today':  [[0 0 0 0 0 0 1 0 0 0 1 1 0 0 2 1 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "new_text = count_vect.transform([\"i read newspaper today i watch tv today\"])\n",
    "print(\"i read newspaper today i watch tv today': \", new_text.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "     In all other approaches text are treated as important. there is no impression of some words in the document being more important than others. TF-IDF (Term Frequenct-inverse document frequency ) solv this problem. It try to quantify the importance of a given word relative to other words. it commonly used in information-retriveal system. \n",
    "     \n",
    "     Idea behind TF-IDF is if the word \"W\" apperas in many times in doument A and not occur much in document B. then word \"W\" is much important to document A.\n",
    "     \n",
    "     TF - measures how often a term or word occurs in given document. This may give biased results when comes to longer documents. to resolve that the number of occurance divided by the length of the doument.\n",
    "     \n",
    "     IDF - Measure the importance of the term across a corpus. when computing TF all terms are given equal importance.but stop words are very common and occur many times in the document and those words are not important. So IDF weighs down the terms that are very common across coprus and weighs up the rare terms.\n",
    "     \n",
    "     IDF = loge(total number of documents in the corpus) (Number of documents with term t in them)\n",
    "     \n",
    "     TF-IDF score  = TF * IDF\n",
    "     \n",
    "     Even though it perform better compare to other methods still suffers from the curs of high dimensionality.\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_tfidf = tfidf.fit_transform(pre_process_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF for all words in the vocabulary [1.69314718 1.69314718 1.28768207 1.28768207 1.28768207 1.28768207\n",
      " 1.28768207 1.69314718]\n",
      "All words in the vocabulary ['and', 'john', 'newspaper', 'read', 'today', 'tv', 'watched', 'yesterday']\n"
     ]
    }
   ],
   "source": [
    "#IDF for all words in the vocabulary\n",
    "print(\"IDF for all words in the vocabulary\",tfidf.idf_)\n",
    "\n",
    "#All words in the vocabulary.\n",
    "print(\"All words in the vocabulary\",tfidf.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF representation for all documents in our corpus\n",
      " [[0.         0.         0.51785612 0.51785612 0.         0.\n",
      "  0.         0.68091856]\n",
      " [0.         0.         0.         0.         0.57735027 0.57735027\n",
      "  0.57735027 0.        ]\n",
      " [0.45212331 0.45212331 0.34385143 0.34385143 0.34385143 0.34385143\n",
      "  0.34385143 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"TFIDF representation for all documents in our corpus\\n\",bow_tfidf.toarray()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i read newspaper today i watch tv today':  [[0.         0.         0.37796447 0.37796447 0.75592895 0.37796447\n",
      "  0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "new_text = tfidf.transform([\"i read newspaper today i watch tv today\"])\n",
    "print(\"i read newspaper today i watch tv today': \", new_text.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Representations\n",
    "    There are some key drawbacks in the vectorization method. To over come that method to learn low-dimentional representation were devised. \n",
    "    they use nural netwrok architecture to create dense, low dimnetional representation of words and texts. \n",
    "    \n",
    "### Distributional similarity\n",
    "    This is called meaning understood by context. ex: \"MJ Rocks\" - Rocks literally meaning stones but in this context it means good.\n",
    "    \n",
    "### Distributional hypothesis\n",
    "    Word that occur in similar context have similar meanings. cat, mouse. is having similar context that's animal and their characterstics. according to distribuationl hypothesis there should be strong similarities between the meaning of these two words.\n",
    "    \n",
    "### Distributional representation\n",
    "    \n",
    "    \n",
    "### Distributed representation\n",
    "\n",
    "\n",
    "### Embedding\n",
    "    Embedding is a mapping between vector space coming from distibutional representation to vector space coming from distributed representation.\n",
    "\n",
    "### Vector semantics\n",
    "    Set of NLP methods that aim to learn the word representations based on distributional properties words in a large corpus.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embessings\n",
    "    Lets just say we give word \"Cat\" distributionally similar words could be other animals either domostic or wild animals.  Neural network word2vec model based on distributional similarity can capture word analogy relationship \"king-man+woman = queen\". when we learn semantically rich relationships word2vec ensures that the learned word representation are low dimentional.Thease representaion are called embeddings.\n",
    "    \n",
    "    Conceptually, Word2vec takes a large corpus of text as input and “learns” to represent the words in a common vector space based on the contexts in which they appear in the corpus. Given a word w and the words appearing in its context C, how do we find the vector that best represents the meaning of the word? For every word w in corpus, we start with a vector v initialized with random values. The Word2vec model refines the values in v by predicting v , given the vectors for words in the context C. It does this using a two-layer neural network.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Trained Word embeddings\n",
    "    Train our own embedding is pretty expensive in both time and computing. Some one already trained embedding on large cporpus such as  wikipedia, news article or entier web. these contain key value pair where key represent the words and value represents corresponding word vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
