{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Pipeline\n",
    "    below are the series of stesp involved in building any NLP model. this step by step process is called pipeline. this process may not linear in nature.\n",
    "\n",
    "    * Data acquisition\n",
    "    * Text cleaning\n",
    "    * Pre-Processing\n",
    "    * Feature engineering\n",
    "    * Modeling\n",
    "    * Evaluvation\n",
    "    * Deployment\n",
    "    * Monitoring and model updating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data acquisition\n",
    "    First step in the process of developing any NLP system is to collect data relevant to the task.\n",
    "\n",
    "    * Public dataset - Open sourced data on internet,\n",
    "    * Scrape data - scrape websites on the internet to collect the data.\n",
    "    * Product intervention -  gather information from product iteself,\n",
    "    * Data augmentation - Create data from existing datset.\n",
    "        * Synonym replacement - randomly choose N words without stop word and replace the word with synonym.(synsets in wordnet)\n",
    "        * Back translation - (english -> german -> english)\n",
    "        * TF-IDF based word replacement\n",
    "        * Bigram Flipping\n",
    "        * Replacing entities - replace person name, location, title etc.,\n",
    "        * Adding noise to data - (spelling error)\n",
    "        * Snorkel - build train data automatically\n",
    "        * Easy data augmentation & NLP Aug - Packages to create synthetic sample\n",
    "        * Active learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Extraction and cleaning\n",
    "    Process of extracting raw text from the input data by removing all other non text information.\n",
    "    \n",
    "    * HTML Parsing and cleanup\n",
    "    * Unicode normalizaion\n",
    "    * Spelling correction\n",
    "    * System specific error correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Html parsing and cleanup example stack overflow \n",
    "#### BS4 and scrapy are some interesting libraries to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import nessary packages\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://stackoverflow.com/questions/415511/how-to-get-the-current-time-in-python\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = urlopen(url).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = soup.find(\"div\", {\"class\": \"question\"})\n",
    "question_text = question.find(\"div\", {\"class\": \"js-post-body\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = soup.find(\"div\", {\"class\": \"answer\"})\n",
    "answer_text = answer.find(\"div\", {\"class\": \"js-post-body\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question : What is the module/method used to get the current time?\n",
      "Answer : Use:\n",
      ">>> import datetime\n",
      ">>> datetime.datetime.now()\n",
      "datetime.datetime(2009, 1, 6, 15, 8, 24, 78915)\n",
      "\n",
      ">>> print(datetime.datetime.now())\n",
      "2009-01-06 15:08:24.789150\n",
      "\n",
      "And just the time:\n",
      ">>> datetime.datetime.now().time()\n",
      "datetime.time(15, 8, 24, 78915)\n",
      "\n",
      ">>> print(datetime.datetime.now().time())\n",
      "15:08:24.789150\n",
      "\n",
      "See the documentation for more information.\n",
      "To save typing, you can import the datetime object from the datetime module:\n",
      ">>> from datetime import datetime\n",
      "\n",
      "Then remove the leading datetime. from all of the above.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Question : {question_text.get_text().strip()}\")\n",
    "\n",
    "print(f\"Answer : {answer_text.get_text().strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unicode Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'I love \\xf0\\x9f\\x8c\\xb9 ! Shall we book a to gizza? \\xe2\\x8c\\x9a'\n"
     ]
    }
   ],
   "source": [
    "text = \"I love ðŸŒ¹ ! Shall we book a to gizza? âŒš\"\n",
    "Text = text.encode(\"utf-8\")\n",
    "print(Text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spelling correction\n",
    "    We can use third party spelling correction api to save time.or we can build our own spell checker using a huge dictionary of words from specific language. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### System specific error correction\n",
    "    Textual data can come from various source not only from internet raw html. it can be come from pdf or OCR scanned images. in this case there are some great python packages out there to extract data from pdf to text format. this is due to different pdf having different encoding format. \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Proccessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    To extract specific and relevant information from raw text is one of crucial steps in NLP pipeline. All NLP softwars typically works at sentence level and expect seperation from word level. so we need t split the text into words and sentences before procedding further. below are the common pre processing techique.\n",
    "    \n",
    "    * Preliminaries\n",
    "        * Sentence segmentaion\n",
    "        * Word Tokenization\n",
    "    * Frequent steps\n",
    "        * Stop word removal\n",
    "        * Stemming and lemmatization\n",
    "        * Removing digits/punctuation\n",
    "        * Lowercasing\n",
    "    * Other steps\n",
    "        * Normalization\n",
    "        * Language detection\n",
    "        * Code mixing\n",
    "        * Transliteration\n",
    "    * Advanced Processing\n",
    "        * POS Tagging\n",
    "        * Parsing\n",
    "        * Coreferecnce resolution\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preliminaries\n",
    "    NLP will typically analyzes text by breaking it up into words and sentences. any NLP Pipeline has to start with reliable way to split the text into sentences and further split sentences into words.\n",
    "    \n",
    "    * Sentence segmentation\n",
    "        We can do sentence segmentation by breaking up text into senteces based on full stops and question marks. some abbrevation may break this rule. so we can use NLTK kit to perform both sentence and word tokenization.\n",
    "    \n",
    "    * Word Tokenization\n",
    "        Similar to sentence it will break text into words, rather than sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''since few days ago, Google return a 404 when we try to access someone's Google Photos public albums, \n",
    "          we can only access it if we have a link of one of his albums. Either this is a bug and this will be fixed, \n",
    "          either it's a protection that we need to find how to bypass.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"since few days ago, Google return a 404 when we try to access someone's Google Photos public albums, \\n          we can only access it if we have a link of one of his albums.\", \"Either this is a bug and this will be fixed, \\n          either it's a protection that we need to find how to bypass.\"]\n"
     ]
    }
   ],
   "source": [
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"since few days ago, Google return a 404 when we try to access someone's Google Photos public albums, \\n          we can only access it if we have a link of one of his albums.\", \"Either this is a bug and this will be fixed, \\n          either it's a protection that we need to find how to bypass.\"]\n",
      "['since', 'few', 'days', 'ago', ',', 'Google', 'return', 'a', '404', 'when', 'we', 'try', 'to', 'access', 'someone', \"'s\", 'Google', 'Photos', 'public', 'albums', ',', 'we', 'can', 'only', 'access', 'it', 'if', 'we', 'have', 'a', 'link', 'of', 'one', 'of', 'his', 'albums', '.']\n",
      "[\"since few days ago, Google return a 404 when we try to access someone's Google Photos public albums, \\n          we can only access it if we have a link of one of his albums.\", \"Either this is a bug and this will be fixed, \\n          either it's a protection that we need to find how to bypass.\"]\n",
      "['Either', 'this', 'is', 'a', 'bug', 'and', 'this', 'will', 'be', 'fixed', ',', 'either', 'it', \"'s\", 'a', 'protection', 'that', 'we', 'need', 'to', 'find', 'how', 'to', 'bypass', '.']\n"
     ]
    }
   ],
   "source": [
    "for i in sentences:\n",
    "    print(sentences)\n",
    "    print(word_tokenize(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Frequet Steps\n",
    "    For example a news classification task after perform a sentence/word segmentation. we would have to start thinking about what information is relevent to group the text into sepcific categoried. obviously the stop words(a,an,the etc.,) doesn't carry much weight. so we might need to remove the stop words. removing stop words doen't make difference on the outcome of the probelm. same way upper case and lower case may not make a differece for the problem.  removing pucnctuation and number also common steps involved in pre processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stop words, digit, punctuation removal & Lower case the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to remove stop words, digits and pucuation and lower case the given text.\n",
    "# This 4 steps neither mandatory nor sequential in nature.\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stops_digits(tokens):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    data = [token.lower() for token in tokens if token not in stop_words and not token.isdigit() and token not in punctuation]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(input_text):\n",
    "    results = [remove_stops_digits(word_tokenize(text)) for text in input_text]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stemming and Lemmatization\n",
    "    Stemming is the process of removing prefix and suffix and reducing the word to some base form(bike/bikes = bike) This is done by applying fixed set of rules. some time it may not always end up in a linguistically correct form.  stemming commonly used in search engine to match user queries to relevant document.\n",
    "    \n",
    "    Lemmatization is the process of mapping all the different forms of word to its base word or lemma. Lemmatization requires more lingustic knowledge and modeling and developing efficient lemmatizers remains an open problem in NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car revolut\n"
     ]
    }
   ],
   "source": [
    "# Stemming the text using port stemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "word1, word2 = \"car\", \"revolution\"\n",
    "\n",
    "print(stemmer.stem(word1), stemmer.stem(word2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization using NLTK Wordnet lemmatizer\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\")) # a is for adjective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "better well\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization using Spacy\n",
    "import spacy\n",
    "\n",
    "sp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "token = sp(u'better')\n",
    "\n",
    "for word in token:\n",
    "    print(word.text, word.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Remember that not all of these steps are always necessary, and not all of them are performed in the order in which theyâ€™re discussed here. For example, if we were to remove digits and punctuation, what is removed first may not matter much. However, we typically lowercase the text before stemming. We also donâ€™t remove tokens or lowercase the text before doing lemmatization because we have to know the part of speech of the word to get its lemma, and that requires all tokens in the sentence to be intact. A good practice to follow is to prepare a sequential list of pre-processing tasks to be done after having a clear understanding of how to process our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common pre process step\n",
    "\n",
    "Text -> Sentence tokenzation -> Sentences\n",
    "\n",
    "Sentece -> Lowecasing\n",
    "-> Removal of punctuation\n",
    "        -> Stemming\n",
    "        -> Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other steps\n",
    "    * Text Normaization\n",
    "        when work with data like socil media post, it have different set of rules. spelled different, shorthand forms, emoji etc., so we need to make a representation of text and capture all the variation into one representation.that's called text normalization. it include lower casing the letter, expand the abbsservation, format/remove number etc.,\n",
    "        \n",
    "    * Language detection\n",
    "        All the data we receive fron internert is not in english language. some time it will be in other language so we need create language specific pipline. so when input data comes we can detect the language using python polyglot package and the next step could be based on the language specific pipeline.\n",
    "        \n",
    "    * Code mixing and Translation\n",
    "        this is where the content is non-english language or more than one language . code mixing refers the switching between languages. when people use multipe lnguages in their write up. they use roman numerals, english ,etc., this data needs to be handle \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanced Processing\n",
    "    * POS Tagging\n",
    "        Some un common tasks like identify specific terms lik name/address from large collection of data needed some non-traditional method in pre processing stage. one such method is called POS Tagging. Example Below\n",
    "\n",
    "    * NER - Named Entidity Recognization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text : The\n",
      "              Lemma : the,\n",
      "              POS : DET,\n",
      "              Shpae : Xxx,\n",
      "              is_alpha : True,\n",
      "              is_stop : True\n",
      "Text : James\n",
      "              Lemma : James,\n",
      "              POS : PROPN,\n",
      "              Shpae : Xxxxx,\n",
      "              is_alpha : True,\n",
      "              is_stop : False\n",
      "Text : Bond\n",
      "              Lemma : Bond,\n",
      "              POS : PROPN,\n",
      "              Shpae : Xxxx,\n",
      "              is_alpha : True,\n",
      "              is_stop : False\n",
      "Text : series\n",
      "              Lemma : series,\n",
      "              POS : NOUN,\n",
      "              Shpae : xxxx,\n",
      "              is_alpha : True,\n",
      "              is_stop : False\n",
      "Text : focuses\n",
      "              Lemma : focus,\n",
      "              POS : VERB,\n",
      "              Shpae : xxxx,\n",
      "              is_alpha : True,\n",
      "              is_stop : False\n",
      "Text : on\n",
      "              Lemma : on,\n",
      "              POS : ADP,\n",
      "              Shpae : xx,\n",
      "              is_alpha : True,\n",
      "              is_stop : True\n",
      "Text : a\n",
      "              Lemma : a,\n",
      "              POS : DET,\n",
      "              Shpae : x,\n",
      "              is_alpha : True,\n",
      "              is_stop : True\n",
      "Text : fictional\n",
      "              Lemma : fictional,\n",
      "              POS : ADJ,\n",
      "              Shpae : xxxx,\n",
      "              is_alpha : True,\n",
      "              is_stop : False\n",
      "Text : British\n",
      "              Lemma : british,\n",
      "              POS : ADJ,\n",
      "              Shpae : Xxxxx,\n",
      "              is_alpha : True,\n",
      "              is_stop : False\n",
      "Text : Secret\n",
      "              Lemma : Secret,\n",
      "              POS : PROPN,\n",
      "              Shpae : Xxxxx,\n",
      "              is_alpha : True,\n",
      "              is_stop : False\n",
      "Text : \n",
      "                \n",
      "              Lemma : \n",
      "                ,\n",
      "              POS : SPACE,\n",
      "              Shpae : \n",
      "    ,\n",
      "              is_alpha : False,\n",
      "              is_stop : False\n",
      "Text : Service\n",
      "              Lemma : Service,\n",
      "              POS : PROPN,\n",
      "              Shpae : Xxxxx,\n",
      "              is_alpha : True,\n",
      "              is_stop : False\n",
      "Text : agent\n",
      "              Lemma : agent,\n",
      "              POS : NOUN,\n",
      "              Shpae : xxxx,\n",
      "              is_alpha : True,\n",
      "              is_stop : False\n",
      "Text : created\n",
      "              Lemma : create,\n",
      "              POS : VERB,\n",
      "              Shpae : xxxx,\n",
      "              is_alpha : True,\n",
      "              is_stop : False\n",
      "Text : in\n",
      "              Lemma : in,\n",
      "              POS : ADP,\n",
      "              Shpae : xx,\n",
      "              is_alpha : True,\n",
      "              is_stop : True\n",
      "Text : 1953\n",
      "              Lemma : 1953,\n",
      "              POS : NUM,\n",
      "              Shpae : dddd,\n",
      "              is_alpha : False,\n",
      "              is_stop : False\n",
      "Text : by\n",
      "              Lemma : by,\n",
      "              POS : ADP,\n",
      "              Shpae : xx,\n",
      "              is_alpha : True,\n",
      "              is_stop : True\n",
      "Text : writer\n",
      "              Lemma : writer,\n",
      "              POS : NOUN,\n",
      "              Shpae : xxxx,\n",
      "              is_alpha : True,\n",
      "              is_stop : False\n",
      "Text : Ian\n",
      "              Lemma : Ian,\n",
      "              POS : PROPN,\n",
      "              Shpae : Xxx,\n",
      "              is_alpha : True,\n",
      "              is_stop : False\n",
      "Text : Fleming\n",
      "              Lemma : Fleming,\n",
      "              POS : PROPN,\n",
      "              Shpae : Xxxxx,\n",
      "              is_alpha : True,\n",
      "              is_stop : False\n",
      "Text : ,\n",
      "              Lemma : ,,\n",
      "              POS : PUNCT,\n",
      "              Shpae : ,,\n",
      "              is_alpha : False,\n",
      "              is_stop : False\n",
      "Text : \n",
      "                \n",
      "              Lemma : \n",
      "                ,\n",
      "              POS : SPACE,\n",
      "              Shpae : \n",
      "    ,\n",
      "              is_alpha : False,\n",
      "              is_stop : False\n",
      "Text : who\n",
      "              Lemma : who,\n",
      "              POS : PRON,\n",
      "              Shpae : xxx,\n",
      "              is_alpha : True,\n",
      "              is_stop : True\n",
      "Text : featured\n",
      "              Lemma : feature,\n",
      "              POS : VERB,\n",
      "              Shpae : xxxx,\n",
      "              is_alpha : True,\n",
      "              is_stop : False\n",
      "Text : him\n",
      "              Lemma : -PRON-,\n",
      "              POS : PRON,\n",
      "              Shpae : xxx,\n",
      "              is_alpha : True,\n",
      "              is_stop : True\n",
      "Text : in\n",
      "              Lemma : in,\n",
      "              POS : ADP,\n",
      "              Shpae : xx,\n",
      "              is_alpha : True,\n",
      "              is_stop : True\n",
      "Text : twelve\n",
      "              Lemma : twelve,\n",
      "              POS : NUM,\n",
      "              Shpae : xxxx,\n",
      "              is_alpha : True,\n",
      "              is_stop : True\n",
      "Text : novels\n",
      "              Lemma : novel,\n",
      "              POS : NOUN,\n",
      "              Shpae : xxxx,\n",
      "              is_alpha : True,\n",
      "              is_stop : False\n",
      "Text : and\n",
      "              Lemma : and,\n",
      "              POS : CCONJ,\n",
      "              Shpae : xxx,\n",
      "              is_alpha : True,\n",
      "              is_stop : True\n",
      "Text : two\n",
      "              Lemma : two,\n",
      "              POS : NUM,\n",
      "              Shpae : xxx,\n",
      "              is_alpha : True,\n",
      "              is_stop : True\n",
      "Text : short\n",
      "              Lemma : short,\n",
      "              POS : ADJ,\n",
      "              Shpae : xxxx,\n",
      "              is_alpha : True,\n",
      "              is_stop : False\n",
      "Text : -\n",
      "              Lemma : -,\n",
      "              POS : PUNCT,\n",
      "              Shpae : -,\n",
      "              is_alpha : False,\n",
      "              is_stop : False\n",
      "Text : story\n",
      "              Lemma : story,\n",
      "              POS : NOUN,\n",
      "              Shpae : xxxx,\n",
      "              is_alpha : True,\n",
      "              is_stop : False\n",
      "Text : collections\n",
      "              Lemma : collection,\n",
      "              POS : NOUN,\n",
      "              Shpae : xxxx,\n",
      "              is_alpha : True,\n",
      "              is_stop : False\n",
      "Text : .\n",
      "              Lemma : .,\n",
      "              POS : PUNCT,\n",
      "              Shpae : .,\n",
      "              is_alpha : False,\n",
      "              is_stop : False\n"
     ]
    }
   ],
   "source": [
    "# Pos Tagging\n",
    "\n",
    "import spacy\n",
    "\n",
    "sp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "text = sp(u'''The James Bond series focuses on a fictional British Secret \n",
    "                Service agent created in 1953 by writer Ian Fleming, \n",
    "                who featured him in twelve novels and two short-story collections.''')\n",
    "\n",
    "for token in text:\n",
    "    print(f'''Text : {token.text}\n",
    "              Lemma : {token.lemma_},\n",
    "              POS : {token.pos_},\n",
    "              Shpae : {token.shape_},\n",
    "              is_alpha : {token.is_alpha},\n",
    "              is_stop : {token.is_stop}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced Pre process step\n",
    "\n",
    "Text -> Sentence tokenzation -> Sentences\n",
    "\n",
    "Sentece -> Lowecasing\n",
    "        -> Removal of punctuation\n",
    "        -> Stemming\n",
    "        -> Lemmatization\n",
    "        -> POS Tagging\n",
    "        -> Parsing\n",
    "        -> Coreference Resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "    Wen we perform Ml model we need to feed the pre-processed text into ML Algorithm. Feature enginnering is set of methods that will accomblsh this task. this also called feature extraction. Goal is to capture text into a numeric vector taht can be understood by ML algorithams.\n",
    "    \n",
    "    * Traditional NLP Pipeline\n",
    "    * DL Pipeline\n",
    "    \n",
    "### Traditional NLP Pipeline:\n",
    "        Features are heavily inspired by the task at hand as well as domain knowledge. advanage of handcrafted features is that model remain interpretable. it's possible to quantify exactly how much each feature is influencing the model. (EX : Review sentiment analysis ->calculate pox/neg word in each sentence)\n",
    "        \n",
    "        Doument --> Pre-processing --> Feature Extraction --> Modeling --> Inference --> Output\n",
    "        \n",
    "### DL Pipeline\n",
    "        \n",
    "        handcrafted features may be comes as a bottle neck for the model performence and model development. A noisy feature can potrntially harm the model perfoemance. In DL pipeline raw data is directly fed into the pipeline. where model will learn the features. since it's using all features it will loss interpretability. It's hard to explain the DL predction which is main disadvantage of bussiness usecase. \n",
    "\n",
    "        Document --> Pre-processing --> Dense embeding --> Hidden layer --> Output\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "    * Simple heuristics approach\n",
    "        After cleaning the data we can move on to model building stage. when we have limited date we can use simpler methods and tules. over time with more data and better understanding of the probelm we can add complexity and improve performance. \n",
    "        when data increases exponentially the rule based system can be so comples, managing feature is no longer easy task and it cause lot of errors so at this point the common practice to combine heuristics directly or indirectly into the ML model. There are two ways we can do that.\n",
    "        \n",
    "        * Create a feature from  the heuristic for ML Model\n",
    "        * Pre Process your inpit to the ML Model\n",
    "\n",
    "    * Ensemble and stacking\n",
    "        It's common practice is to have collection of ML models, oftern dealing with different aspect of the problem. ther are two ways of doing this\n",
    "        we can feed one model's output as input for another model. it's like sequentially ging from one model to another model to obtain the final\n",
    "        output. it's called model stacking. We can also pool predictions from multiple models and make final prediction it's called model ensembling.\n",
    "        \n",
    "        Training Data --> Ensemble (level 1 multiple model) --> Input level 2 --> meta model --> Final prediction.\n",
    "        \n",
    "    * Better Feature Engineering\n",
    "    * Transfer Learning\n",
    "        In some case external knowledge is needed beyond the dataset for the task to understand the problem. \n",
    "    * Reapplying Heuristics\n",
    "    \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Data | Approach |\n",
    "| :------------- | :----------\n",
    "| Large Data  | Can apply techiques like DL, where it reuires more data |\n",
    "| Small Data   | Need to start with rule based or tranditional ML. also we can apply transfer learning |\n",
    "| Data Quality is poor and heterogeneous in nature  | More data cleaning and pre-processing might be required |\n",
    "| Data Quality is good   | Can directly apply off the self algoritham or Cloud API |\n",
    "| Data consists of full length documets  | Choose right strategy to break the document into lower level like paragraph |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "    Evaluation is to measure how good the model. Good means model's performance on unseen data.  there are two crucial step involved in this process\n",
    "    * Choosig right metric for evaluvation\n",
    "    * Following the right evaluvation process.\n",
    "    \n",
    "    Evaluvation also are two types: Intrinsic and extrinsic\n",
    "    \n",
    "### Intrinsic Evaluation\n",
    "    In Many case this evaluation metric can be automated, but for some problem cases like machine translation or summarization automate evaluvation is not always possible.\n",
    "    \n",
    "| Data | Approach | Applications |\n",
    "| :------------- | :----------| :------------- |\n",
    "| Accuracy | Used when the output variable is categorical or discrete. It denotes the fraction of times the model makes correct prdiction compare to total prediction it makes.|  classification tasks, parapharse detection etc.,|\n",
    "| Precision | How exact the model's predictions are, given all the positive case how many can the model classify correctly? | Used in various classification tasks,especially in cases wheremistakes in a positive class are more costly than mistakes in a negative class, e.g., disease predictions in healthcare. |\n",
    "| Recall | It's complementary to precision. it capture how well the model recall positive class, Given all the positive prediction it makes, how many of them are indeed positive?  | Used in classification tasks, especially where retrieving positive results is more important, e.g., ecommerce search and other informationretrieval tasks. |\n",
    "| F1 Score | combines precision and recall to give single metric. which also capture trade-off between precision and recall. F1 = (2 X Precision X Recall)/ (Precision + Recall ) | Used simultaneously with accuracy in most of the classification tasks. It is also used in sequencelabeling tasks, such as entity extraction,retrieval-based questions answering, etc. |\n",
    "| AUC  | Count of +ve prediction that are correct vs count of +ve prediction that are incorrect as we vary the threshold for prediction | Used to measure the quality of a model independent of the prediction threshold. It is used to find the optimal prediction threshold for a classification task.|\n",
    "| MRR (Mean reciprocal Rank) | Responses retrives given their probability of correctness. It's the mean of the reciprocal of the ranks of the retrives results.| heavily on inforamtion retrival task. |\n",
    "| MAP (Mean average precision) | It calculates mean precision across each retrives result. | information retrival task |\n",
    "| RMSE (Mean reciprocal Rank) | capture a model performance in a real value prediction task. calaculates the square root of the mean of the squared errors for each data point | used in the case of regression problem |\n",
    "| MAPE (Mean reciprocal Rank) | used whrn output varible is a continous variable. It is the average of absolute % error for each data point.| Used to test the performace of regression model. |\n",
    "| BLEU (Mean reciprocal Rank) | capture the amount of n-gram overlap b/w the output sentence and the refernce ground truth sentences | text translation, text summarization etc., |\n",
    "| METEOR (Mean reciprocal Rank) | A precision-based metric to measure the quality of text generated. It fixes some of the drawbacks of BLEU, such as exact word matching while calculating precision. METEOR allows synonyms and stemmed words to be matched with the reference word. | Used in machine translation |\n",
    "| ROUGE (Mean reciprocal Rank) | Another metric to compare quality of generated text with respect to a reference text. As opposed to BLEU, it measures recall. | since it measured recall, it mainly used on summarization tasks where it's important to recall how many words a model can recall. |\n",
    "| Perplexity (Mean reciprocal Rank) | A probabilistic measure that captures how confused an NLP model is. Itâ€™s derived from the cross-entropy in a next word prediction task | evaluvate language model, also used on the language generation tasks such as dialog generation. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In calssification task most commonly used evaluvation method is a confusion matrix. Confusion metrics is in a turn usesd to compute metrics such as precision, recll, F1 score. \n",
    "\n",
    "Ranking tasks like information retrival mostly used raning based methos like MRR, MAP \n",
    "\n",
    "When it comes to text-generation tasks, there are a number of metrics that are used, depending on the task. Even though BLEU and METEOR are good metrics for machine translation, they may not be good metrics when applied to other generation tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extrinsic Evaluation\n",
    "    It's focus on evaluvating the model performance based on the final objective. Exterinsic evaluvation often include project stakeholders outside the AI team. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment\n",
    "    NLP Model typically deployed as a web service. \n",
    "\n",
    "## Monitoring\n",
    "    Like any s/w engg the externsive s/w testing has to be done before final deployment and the model performance is monitiored constantly after the deployment.\n",
    "    \n",
    "## Model Updating\n",
    "    Once the model is deployed and start gathering new data, we iterate the model based on new data.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
